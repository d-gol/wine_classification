{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.6.3\n",
    "# import statements\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global declarations\n",
    "np.set_printoptions(precision=2, suppress=True) # remove scientific notation in printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return all unique elements and number of occurence from the input array, as a dict\n",
    "def make_dict_counts(Y, start, end):\n",
    "    unique, counts = np.unique(Y, return_counts=True)\n",
    "    dict_counts = dict(zip(unique, counts))\n",
    "        \n",
    "    for key in range(start, end):\n",
    "        if not key in dict_counts:\n",
    "            dict_counts[key] = 0\n",
    "            \n",
    "    return dict_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file and split data on X and Y\n",
    "def get_X_Y_counts(filename):\n",
    "    data = np.loadtxt(open(filename, \"rb\"), delimiter=\";\", skiprows=1)\n",
    "    X, Y = np.split(data, [-1], axis=1)\n",
    "    Y = Y.astype(int)\n",
    "    \n",
    "    dict_counts = make_dict_counts(Y, 0, 10)\n",
    "            \n",
    "    return X, Y, dict_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train and test set by taking equal percentage of examples from each class\n",
    "def make_train_test(X, Y, dict_counts, test_perc):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    \n",
    "    dict_counts_temp = dict_counts.copy()\n",
    "    \n",
    "    for i in range(0, X.shape[0]):\n",
    "        if dict_counts_temp.get(Y[i][0], 0) > test_perc * dict_counts.get(Y[i][0], 0):\n",
    "            X_train.append(X[i])\n",
    "            Y_train.append(Y[i])\n",
    "        else:\n",
    "            X_test.append(X[i])\n",
    "            Y_test.append(Y[i])\n",
    "        dict_counts_temp[Y[i][0]] -= 1\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    \n",
    "    dict_counts_train = make_dict_counts(Y_train, 1, 10)\n",
    "    dict_counts_test = make_dict_counts(Y_test, 1, 10)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, dict_counts_train, dict_counts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate data for red and white wine\n",
    "def concatinate_train_test(X_red_train, Y_red_train, X_red_test, Y_red_test,\n",
    "                           X_white_train, Y_white_train, X_white_test, Y_white_test):\n",
    "    X_train = np.append(X_red_train, X_white_train, axis=0)\n",
    "    Y_train = np.append(Y_red_train, Y_white_train, axis=0)\n",
    "    X_test = np.append(X_red_test, X_white_test, axis=0)\n",
    "    Y_test = np.append(Y_red_test, Y_white_test, axis=0)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print info about shapes of red and white wine, and in total\n",
    "def print_shapes(X, Y, X_train, Y_train, X_test, Y_test, red_white):\n",
    "    print(str(red_white) + \" wine:\")\n",
    "    print(\"X shape: \" + str(X.shape))\n",
    "    print(\"Y shape: \" + str(Y.shape))\n",
    "    print(\"Train set X shape: \" + str(X_train.shape))\n",
    "    print(\"Train set Y shape: \" + str(Y_train.shape))\n",
    "    print(\"Test set X shape: \" + str(X_test.shape))\n",
    "    print(\"Test set Y shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shapes of training and test sets\n",
    "def print_result_shapes(X_train, Y_train, X_test, Y_test):\n",
    "    print(\"Split data:\")\n",
    "    print(\"Train set X shape: \" + str(X_train.shape))\n",
    "    print(\"Train set Y shape: \" + str(Y_train.shape))\n",
    "    print(\"Test set X shape: \" + str(X_test.shape))\n",
    "    print(\"Test set Y shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of data over the classes\n",
    "def plot_counts(dict_counts, title, i):\n",
    "    #plt.figure(figsize=(8, 40))\n",
    "    plt.subplot(6, 1, i)\n",
    "    plt.bar(dict_counts.keys(), dict_counts.values())\n",
    "    plt.title(title)\n",
    "    labs = [3, 4, 5, 6, 7, 8, 9]\n",
    "    plt.xticks(labs, labs)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of examples')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_stats(dict_counts_red, dict_counts_red_train, dict_counts_red_test,\n",
    "                     dict_counts_white, dict_counts_white_train, dict_counts_white_test):\n",
    "    #plt.figure(1)\n",
    "    plot_counts(dict_counts_red, \"Original Red Wine Data Distribution\", 1)\n",
    "    #plot_counts(dict_counts_red_train, \"Red Wine Train Set Distribution\", 2)\n",
    "    #plot_counts(dict_counts_red_test, \"Red Wine Test Set Distribution\", 3)\n",
    "    \n",
    "    plot_counts(dict_counts_white, \"Original White Wine Data Distribution\", 4)\n",
    "    #plot_counts(dict_counts_white_train, \"White Wine Train Set Distribution\", 5)\n",
    "    #plot_counts(dict_counts_white_test, \"White Wine Test Set Distribution\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an array with n classes, return same array with 2 classes (bad, good)\n",
    "def convert_to_two_classes(Y):\n",
    "    res = Y\n",
    "    for i in range(0, Y.shape[0]):\n",
    "        if Y[i] >= 0 and Y[i] <= 5:\n",
    "            res[i] = 1\n",
    "        else:\n",
    "            res[i] = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read data from filenames and return train and test sets\n",
    "def read_data(foldername, filename_red, filename_white):\n",
    "    test_perc = 0.21\n",
    "    \n",
    "    X_red, Y_red, dict_counts_red = get_X_Y_counts(foldername + \"/\" + filename_red)\n",
    "    X_white, Y_white, dict_counts_white = get_X_Y_counts(foldername + \"/\" + filename_white)\n",
    "    \n",
    "    X_red_train, Y_red_train, X_red_test, Y_red_test, dict_counts_red_train, dict_counts_red_test = make_train_test(X_red, Y_red, dict_counts_red, test_perc)\n",
    "    X_white_train, Y_white_train, X_white_test, Y_white_test, dict_counts_white_train, dict_counts_white_test = make_train_test(X_white, Y_white, dict_counts_white, test_perc)    \n",
    "    \n",
    "    X_train, Y_train, X_test, Y_test = concatinate_train_test(X_red_train, Y_red_train, X_red_test, Y_red_test, X_white_train, Y_white_train, X_white_test, Y_white_test)\n",
    "    \n",
    "    Y_train = convert_to_two_classes(Y_train)\n",
    "    Y_test = convert_to_two_classes(Y_test)\n",
    "    \n",
    "    X_train = preprocessing.scale(X_train)\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "    \n",
    "    return X_train.T, Y_train, X_test.T, Y_test\n",
    "\n",
    "#X_train, Y_train_orig, X_test, Y_test_orig = read_data(\"../wine\", \"winequality-red.csv\", \"winequality-white.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF placeholders for arrays X and Y\n",
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32, shape=[n_x, None], name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=[n_y, None], name = \"Y\")\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters for a deep neural network\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = tf.get_variable(\"W\" + str(l), [layer_dims[l], layer_dims[l-1]], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        parameters['b' + str(l)] = tf.get_variable(\"b\" + str(l), [layer_dims[l], 1], initializer = tf.zeros_initializer())\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation of a deep neural network\n",
    "def forward_propagation_deep(X, parameters):\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        Z = tf.add(tf.matmul(parameters['W' + str(l)], A), parameters['b' + str(l)])\n",
    "        A = tf.nn.relu(Z)\n",
    "        \n",
    "    Z = tf.add(tf.matmul(parameters['W' + str(L)], A), parameters['b' + str(L)])\n",
    "            \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cost of a NN, use_reg determines whether to use regularization or not\n",
    "def compute_cost_deep(Z3, Y, parameters, use_reg):\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    #cost = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits = logits, targets = labels, pos_weight = 1.72))\n",
    "    \n",
    "    if use_reg == True:\n",
    "        beta = 0.005\n",
    "        regularizers = tf.nn.l2_loss(parameters['W1'])\n",
    "        for l in range(2, L + 1):\n",
    "            regularizers += tf.nn.l2_loss(parameters['W' + str(l)])\n",
    "        cost = tf.reduce_mean(cost + beta * regularizers)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a matrix to one hot matrix\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a list of random minibatches from (X, Y)\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    m = X.shape[1] # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data on two classes\n",
    "def split_two_classes(X, Y):\n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "    \n",
    "    X_first = []\n",
    "    Y_first = []\n",
    "    X_second = []\n",
    "    Y_second = []\n",
    "    \n",
    "    for i in range(0, Y.shape[0]):\n",
    "        if Y[i][1] == 1: # because it's one hot!!! with two classes\n",
    "            X_first.append(X[i])\n",
    "            Y_first.append(Y[i])\n",
    "        else:\n",
    "            X_second.append(X[i])\n",
    "            Y_second.append(Y[i])\n",
    "            \n",
    "    X_first = np.array(X_first)\n",
    "    Y_first = np.array(Y_first)\n",
    "    X_second = np.array(X_second)\n",
    "    Y_second = np.array(Y_second)\n",
    "    \n",
    "    return X_first.T, Y_first.T, X_second.T, Y_second.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix of two classes, bad and good wine\n",
    "def plot_confusion_matrix(cm, labs):\n",
    "    plt.matshow(cm, cmap=plt.cm.Blues)\n",
    "    #plt.tight_layout()\n",
    "    labels = ['Bad', 'Good']\n",
    "    plt.xticks(labs, labels)\n",
    "    plt.yticks(labs, labels)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    fmt = ''\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a cost function\n",
    "def plot_cost(costs, learning_rate):\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a network, evaluating performance on a test set, returning parameters of a neural network\n",
    "def train(X_train, Y_train, learning_rate = 0.01, num_epochs = 10000, minibatch_size = 256, print_cost = True):\n",
    "\n",
    "    ops.reset_default_graph() \n",
    "    (n_x, m) = X_train.shape # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0] # n_y : output size\n",
    "    costs = [] \n",
    "\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    \n",
    "    # Deep\n",
    "    layers_dims = (n_x, 20, 20, 20, n_y)\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    Z3 = forward_propagation_deep(X, parameters)\n",
    "    cost = compute_cost_deep(Z3, Y, parameters, True)\n",
    "\n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    X_first, Y_first, X_second, Y_second = split_two_classes(X_train, Y_train)\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0 # Defines a cost related to an epoch\n",
    "            \n",
    "            num_minibatches = int(X_second.shape[1] / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            minibatches_first = random_mini_batches(X_first, Y_first, minibatch_size)\n",
    "            minibatches_second = random_mini_batches(X_second, Y_second, minibatch_size)\n",
    "\n",
    "            #for minibatch in minibatches:\n",
    "            for i in range(0, len(minibatches_first)):\n",
    "                minibatch_first = minibatches_first[i]\n",
    "                minibatch_second = minibatches_second[i]\n",
    "                # Select a minibatch\n",
    "                (minibatch_first_X, minibatch_first_Y) = minibatch_first\n",
    "                (minibatch_second_X, minibatch_second_Y) = minibatch_second\n",
    "                \n",
    "                minibatch_X = np.append(minibatch_first_X, minibatch_second_X, axis=1)\n",
    "                minibatch_Y = np.append(minibatch_first_Y, minibatch_second_Y, axis=1)\n",
    "                \n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            #_, epoch_cost = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train})\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plot_cost(costs, learning_rate)\n",
    "        \n",
    "        # Save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        #print (\"Parameters have been trained and saved!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Get average from the results\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", str(round(accuracy.eval({X: X_train, Y: Y_train}), 3)))\n",
    "\n",
    "        test_tensor = tf.argmax(Z3)\n",
    "        \n",
    "        # Conf matrix\n",
    "        labs = [0, 1]\n",
    "        cm = confusion_matrix(np.argmax(Y_train, 0), test_tensor.eval({X: X_train}), labels=labs)\n",
    "        plot_confusion_matrix(cm, labs)\n",
    "        tp, fn, fp, tn = cm.ravel()\n",
    "        recall = tp / (tp + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        print(\"Train class 0 recall: \" + str(round(recall, 3)))\n",
    "        print(\"Train class 1 recall: \" + str(round(tn / (tn + fp), 3)))\n",
    "        #print(\"Train precision: \" + str(precision))\n",
    "        #print(\"Train F-measure: \" + str(2 * precision * recall / (precision + recall)))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run and evaluate trained neural network on a test set\n",
    "def sim(X_test, Y_test, parameters):\n",
    "    (n_x, m) = X_test.shape # (n_x: input size, m : number of examples in the test set)\n",
    "    n_y = Y_test.shape[0] # n_y : output size\n",
    "    \n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    Y_pred = forward_propagation_deep(X, parameters)\n",
    "    labs = [0, 1]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        correct_prediction = tf.equal(tf.argmax(Y_pred), tf.argmax(Y))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"Test Accuracy:\", str(round(accuracy.eval({X: X_test, Y: Y_test}), 3)))\n",
    "\n",
    "        test_tensor = tf.argmax(Y_pred)\n",
    "        cm = confusion_matrix(np.argmax(Y_test, 0), test_tensor.eval({X: X_test}), labels=labs)\n",
    "        plot_confusion_matrix(cm, labs)\n",
    "        tp, fn, fp, tn = cm.ravel()\n",
    "        recall = tp / (tp + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        print(\"Test class 0 recall: \" + str(round(recall, 3)))\n",
    "        print(\"Test class 1 recall: \" + str(round(tn / (tn + fp), 3)))\n",
    "        #print(\"Test precision: \" + str(precision))\n",
    "        #print(\"Test F-measure: \" + str(2 * precision * recall / (precision + recall)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train: (11, 5139)\n",
      "Shape of Y train: (11, 1358)\n",
      "Shape of X test: (2, 5139)\n",
      "Shape of Y test: (2, 1358)\n",
      "\n",
      "Training started:\n",
      "Train Accuracy: 0.796\n",
      "Train class 0 recall: 0.781\n",
      "Train class 1 recall: 0.822\n",
      "\n",
      "Evaluating results on the test set:\n",
      "Test Accuracy: 0.771\n",
      "Test class 0 recall: 0.799\n",
      "Test class 1 recall: 0.723\n"
     ]
    }
   ],
   "source": [
    "# script to run\n",
    "plt.close(\"all\")\n",
    "%matplotlib qt\n",
    "X_train, Y_train_orig, X_test, Y_test_orig = read_data(\"../wine\", \"winequality-red.csv\", \"winequality-white.csv\")\n",
    "n_classes = 2\n",
    "Y_train = convert_to_one_hot(Y_train_orig.T, n_classes)\n",
    "Y_test = convert_to_one_hot(Y_test_orig.T, n_classes)\n",
    "\n",
    "print(\"Shape of X train: \" + str(X_train.shape))\n",
    "print(\"Shape of Y train: \" + str(X_test.shape))\n",
    "print(\"Shape of X test: \" + str(Y_train.shape))\n",
    "print(\"Shape of Y test: \" + str(Y_test.shape))\n",
    "\n",
    "print(\"\\nTraining started:\")\n",
    "parameters = train(X_train, Y_train, num_epochs = 1000, learning_rate = 0.01, print_cost = False)\n",
    "\n",
    "print(\"\\nEvaluating results on the test set:\")\n",
    "sim(X_test, Y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
